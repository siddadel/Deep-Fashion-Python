{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrustLab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fWFB09kLy8VGYM7zR03YPmabmR61We8x",
      "authorship_tag": "ABX9TyNqEjar0qg91/N4UEDbcodE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddadel/Deep-Fashion-Python/blob/master/TrustLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install warcio\n",
        "# !pip install langdetect"
      ],
      "metadata": {
        "id": "x8LX7P24ODb2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import gzip\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "import threading\n",
        "from warcio.archiveiterator import ArchiveIterator\n",
        "import re\n",
        "import requests\n",
        "import sys\n",
        "import calendar\n",
        "import re\n",
        "import datetime"
      ],
      "metadata": {
        "id": "R8_aqwikYG8k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C590mfhrNv3h"
      },
      "outputs": [],
      "source": [
        "#presently we support only english\n",
        "#Potential for BIAS!\n",
        "valid_locale = [\"en\"]\n",
        "valid_tlds = [\".com/\"]\n",
        "valid_domains = [\"https://www.cnn.com/\"]\n",
        "\n",
        "main_regex = re.compile(\"(covid|pandemic|covid-19)\", re.IGNORECASE)\n",
        "aux_regex = re.compile(\"(economy|cost|inflation|loss|economic|costs)\", re.IGNORECASE)\n",
        "\n",
        "def find_relevant(contents, uri, month, id):\n",
        "  soup = BeautifulSoup(contents, 'html5lib')\n",
        "  [s.decompose() for s in soup(\"script\")]  # remove <script> elements\n",
        "  body_text = soup.body.get_text().lower()\n",
        "  if len(body_text)!=0 and is_lang_valid(body_text):\n",
        "      m1 = main_regex.search(body_text)\n",
        "      if(m1):\n",
        "        m2 = aux_regex.search(body_text)\n",
        "        if(m2):\n",
        "          print((\"%s,%s\")%(calendar.month_abbr[month], uri))\n",
        "\n",
        "def is_tld_valid(uri):\n",
        "  for tld in valid_tlds:\n",
        "    if tld in uri:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def is_valid_domain(uri):\n",
        "  return True\n",
        "  # for domain in valid_domains:\n",
        "  #   if domain in uri:\n",
        "  #     return True\n",
        "  # return False\n",
        "\n",
        "def is_lang_valid(body_text):\n",
        "  lang = detect(body_text)\n",
        "  return lang in valid_locale\n",
        "\n",
        "def get_valid_uri(record):\n",
        "  uri = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
        "  if record.rec_type != \"warcinfo\" and is_tld_valid(uri) and is_valid_domain(uri):\n",
        "    # pprint(vars(record))\n",
        "    return uri\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def parse_segment(file_name):\n",
        "  try:\n",
        "    id = 0\n",
        "    stream = requests.get(file_name, stream=True).raw\n",
        "    for record in ArchiveIterator(stream):\n",
        "      tz = record.rec_headers.get_header(\"WARC-Date\")\n",
        "      # date_time_obj = datetime.strptime(tz, '%y-%m-%dT%H:%M:%SZ')\n",
        "      tokens = tz.split('-')\n",
        "      uri = get_valid_uri(record)\n",
        "      if uri:\n",
        "        contents = (\n",
        "              record.content_stream()\n",
        "              .read()\n",
        "              .decode(\"utf-8\", \"replace\")\n",
        "          )\n",
        "        find_relevant(contents, uri,int(tokens[1]), id)\n",
        "        id+=1\n",
        "  except Exception as e:\n",
        "     pass\n",
        "    # print(\"{} for {}\".format(e, uri))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i_s = [5, 10, 16, 24, 29, 34, 40, 45, 50]\n",
        "i_s.reverse()\n",
        "for i in i_s:\n",
        "  file_name = \"https://data.commoncrawl.org/crawl-data/CC-MAIN-2020-{0:0=2d}/warc.paths.gz\".format(i)\n",
        "  stream = requests.get(file_name, stream=True).raw\n",
        "  with gzip.open(stream, 'rt') as f:\n",
        "    file_content = f.read()\n",
        "    threads = []\n",
        "    for line in file_content.splitlines():\n",
        "        link = \"http://commoncrawl.s3.amazonaws.com/{}\".format(line)\n",
        "        thread = threading.Thread(target=parse_segment, args=(link,), daemon=True)\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "\n",
        "    for thread in threads:\n",
        "        thread.join()  "
      ],
      "metadata": {
        "id": "DsgdWj0SbnIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_ik-jI2SQ6Hv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}